{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNXs8d7zmTyNUmsAfe4BOjv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FabysMV/Detection_depress/blob/main/depression_detect.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Xn_WHvZMBaHL"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import pandas\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar los datos .csv\n",
        "#1.- -\tTamaño promedio (y desviación estándar) de\n",
        "# las transcripciones, en palabras y en tiempo; por intervenciones o entrevista completa\n",
        "\n",
        "#text = pandas.read_csv()\n",
        "\n",
        "\n",
        "# Funciones de calculos\n",
        "# Los tiempos y la transcripción ya debe de estar filtrada\n",
        "def tam_prom(trans, time1, time2):\n",
        "    \"\"\"devuelve el tamaño promedio y desviación estándar de las palabras y el tiempo (type = list)\n",
        "        en la primera posicion se encuentra el promedio y en la segunda la desviación estándar\"\"\"\n",
        "    dat_time = []  # datos de tiempo\n",
        "    dat_pal = []  # datos de las palabras\n",
        "\n",
        "    # calculamos para las palabras\n",
        "    tam = nltk.word_tokenize(trans)  # devuelve lista de palabras\n",
        "    dat_pal[0] = sum(tam)/len(tam)  # promedio de palabras\n",
        "    dat_pal[1] = np.sqrt(np.var(tam))  # desviación estandar\n",
        "\n",
        "    # calculamos para el tiempo\n",
        "    time_delta = [t1 - t2 for t1, t2 in zip(time1, time2)]  # duración de cada frase\n",
        "    dat_time[0] = sum(time_delta)/len(time_delta)\n",
        "    dat_time[1] = np.sqrt(np.var(time_delta))\n",
        "\n",
        "############################################################################################################\n",
        "\n",
        "def tam_word(trans):\n",
        "    \"\"\" Devuelve el promedio de caracteres usados en la trasncripción\"\"\"\n",
        "    palabras = nltk.word_tokenize(trans)  # devuelve lista de palabras\n",
        "    prom = [len(p) for p in palabras]  # lista con tamaño de las palabras\n",
        "    return sum(prom) / len(palabras)  # promedio de tamaño de palabras\n",
        "\n",
        "\n",
        "def tam_sentence(trans):\n",
        "    \"\"\"Devuelve el promedio y la desviación estándar de palabras por oración\n",
        "    type = list donde el primer elemento es el promedio y el segundo la desviación estándar\"\"\"\n",
        "    palabras = nltk.word_tokenize(trans)  # devuelve lista de palabras\n",
        "    oracion = nltk.tokenize.sent_tokenize(trans)\n",
        "\n",
        "    # Desviación estándar de las palabras por oración\n",
        "    len_oracion = [len(nltk.word_tokenize(orac)) for orac in oracion]\n",
        "    return [len(palabras)/len(oracion), np.sqrt(np.var(len_oracion))]\n",
        "\n",
        "\n",
        "def rel_oracion_palabra(trans):\n",
        "    \"\"\"Devuelve la relacion entre cantidad de oraciones y palabras\n",
        "    type = list\n",
        "    Primer elemento = Tamaño de las oraciones / Tamaño del vocabulario\n",
        "    Segundo elemento = Número de oraciones / Tamaño del vocabulario\"\"\"\n",
        "    palabras = nltk.word_tokenize(trans)  # devuelve lista de palabras\n",
        "\n",
        "    # Tamaño de las oraciones / Tamaño del vocabulario\n",
        "    oraciones = nltk.tokenize.sent_tokenize(trans)\n",
        "    tam_oraciones = [len(o) for o in oraciones]\n",
        "    tam_vocab = len(set(palabras))\n",
        "\n",
        "    # Número de oraciones / Tamaño del vocabulario\n",
        "    num_or = len(oraciones)\n",
        "    return [tam_oraciones/tam_vocab, num_or/tam_vocab]\n",
        "##\n",
        "\n",
        "\n",
        "def vocab_rich(trans):\n",
        "    \"\"\"Devuelve una lista con el Número de palabras / tamaño del vocabulario y el Número de Hapax / Tamaño del vocabulario en e\"\"\"\n",
        "    palabras = nltk.word_tokenize(trans)  # devuelve lista de palabras\n",
        "    tam_vocab = len(set(palabras))  # tamaño de vocabulario\n",
        "\n",
        "    #\tNúmero de palabras / tamaño del vocabulario\n",
        "    # o\tNúmero de Hapax / Tamaño del vocabulario\n",
        "    # Para encontrar los Hapaxes primero hay que aplicar la distribución de frecuencia\n",
        "    fdist = nltk.FreqDist(trans)\n",
        "    hapaxes = fdist.hapaxes()\n",
        "    return [len(palabras/tam_vocab), len(hapaxes)]\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------------\n",
        "# importar data set desde google"
      ],
      "metadata": {
        "id": "KDBQnLwuBi4o"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}